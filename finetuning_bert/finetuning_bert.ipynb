{"cells":[{"cell_type":"markdown","metadata":{"id":"hz6deuPyRjhw"},"source":["#Fine-tuning Bert for the HUHU Shared Task\n","##Approach\n","In this approach we trained, for subtask 1 and each of the categories of subtask 2A, one fine-tuned version of the bert_base_multilingual_cased model for binary classification.<br>\n","The code to implement the network was adapted from [this example](https://github.com/marcellusruben/medium-resources/blob/main/Text_Classification_BERT/bert_medium.ipynb).<br>\n","The Shared Task consisted of the following three subtasks (as explained in the official website).\n","##Subtask 1: HUrtful HUmour Detection\n","The first subtask consists in determining whether a prejudicial tweet is intended to cause humour. Participants will have to distinguish between tweets that using humour express prejudice and tweets that express prejudice without using humour.\n","##Subtask 2A: Prejudice Target Detection\n","Taking into account the following minority groups: Women and feminists, LGBTIQ community and Immigrants, racially discriminated people, and overweight people, participants are asked to identify the targeted groups on each tweet as a multilabel classification task.\n","##Subtask 2B: Degree of Prejudice Prediction\n","The third subtask consists of predicting on a continuous scale from 1 to 5 to evaluate how prejudicial the message is on average among minority groups."]},{"cell_type":"markdown","metadata":{"id":"Kp4XcwuNSPYy"},"source":["##Imports"]},{"cell_type":"markdown","metadata":{"id":"Q1pkBky6UrYm"},"source":["To run the following code, it is necessary to have the following files in the root of your directory structure:\n","\n","*   huhu_split_train.csv\n","*   huhu_split_dev.csv\n","\n","In our experiments, we decided to make our own internal 90%-10% split of the training set that was made available by the organizers. This training set can be downloaded from [zenodo](https://zenodo.org/record/7967255)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXr1kd9j_GU9"},"outputs":[],"source":["import pandas as pd\n","\n","train_set = pd.read_csv('huhu_split_train.csv')\n","dev_set = pd.read_csv('huhu_split_dev.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDzr5yn4PLO8"},"outputs":[],"source":["!pip install transformers\n","\n","import torch\n","import numpy as np\n","from transformers import BertTokenizer, BertModel\n","from torch import nn\n","from torch.optim import Adam\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"markdown","metadata":{"id":"sNmbdy45PNAs"},"source":["##Datasets"]},{"cell_type":"markdown","metadata":{"id":"KETYtYkHVS2D"},"source":["###Dataset (Task 1 and 2A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xob7O4vPcyo"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","labels = {0:0,\n","          1:1\n","          }\n","\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, train_set, label):\n","\n","        self.labels = [labels[iterLabel] for iterLabel in train_set[label]]\n","        self.texts = [tokenizer(text,\n","                               padding='max_length', max_length = 512, truncation=True,\n","                                return_tensors=\"pt\") for text in train_set['tweet']]\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return np.array(self.labels[idx])\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of inputs\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        return batch_texts, batch_y"]},{"cell_type":"markdown","metadata":{"id":"nEjdmoqqc7y7"},"source":["###Validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3jhKWXfc970"},"outputs":[],"source":["np.random.seed(112)\n","train_set_humor, val_set_humor = np.split(train_set.sample(frac=1, random_state=42),\n","                                     [int(.9*len(train_set))])\n","\n","print(len(train_set_humor),len(val_set_humor), len(dev_set))"]},{"cell_type":"markdown","metadata":{"id":"YOS1kDS2cq-4"},"source":["##BertClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tu8tNF3zcs_k"},"outputs":[],"source":["class BertClassifier(nn.Module):\n","\n","    def __init__(self, dropout=0.5):\n","\n","        super(BertClassifier, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(768, 2)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input_id, mask):\n","\n","        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n","        dropout_output = self.dropout(pooled_output)\n","        linear_output = self.linear(dropout_output)\n","        final_layer = self.relu(linear_output)\n","\n","        return final_layer"]},{"cell_type":"markdown","metadata":{"id":"ls_EZgj3mw4Z"},"source":["##Training loop"]},{"cell_type":"markdown","metadata":{"id":"6qLX6DK1g40N"},"source":["###Tasks 1 and 2A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wOqxbFQmzbU"},"outputs":[],"source":["def train(model, train_data, val_data, label, learning_rate, epochs):\n","\n","    train, val = Dataset(train_data, label), Dataset(val_data, label)\n","\n","    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n","    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = Adam(model.parameters(), lr= learning_rate)\n","\n","    if use_cuda:\n","\n","            model = model.cuda()\n","            criterion = criterion.cuda()\n","\n","    for epoch_num in range(epochs):\n","\n","            total_acc_train = 0\n","            total_loss_train = 0\n","\n","            for train_input, train_label in tqdm(train_dataloader):\n","\n","                train_label = train_label.to(device)\n","                mask = train_input['attention_mask'].to(device)\n","                input_id = train_input['input_ids'].squeeze(1).to(device)\n","\n","                output = model(input_id, mask)\n","\n","                batch_loss = criterion(output, train_label.long())\n","                total_loss_train += batch_loss.item()\n","\n","                acc = (output.argmax(dim=1) == train_label).sum().item()\n","                total_acc_train += acc\n","\n","                model.zero_grad()\n","                batch_loss.backward()\n","                optimizer.step()\n","\n","            total_acc_val = 0\n","            total_loss_val = 0\n","\n","            with torch.no_grad():\n","\n","                for val_input, val_label in val_dataloader:\n","\n","                    val_label = val_label.to(device)\n","                    mask = val_input['attention_mask'].to(device)\n","                    input_id = val_input['input_ids'].squeeze(1).to(device)\n","\n","                    output = model(input_id, mask)\n","\n","                    batch_loss = criterion(output, val_label.long())\n","                    total_loss_val += batch_loss.item()\n","\n","                    acc = (output.argmax(dim=1) == val_label).sum().item()\n","                    total_acc_val += acc\n","\n","            print(\n","                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n","                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n","                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n","                | Val Accuracy: {total_acc_val / len(val_data): .3f}')"]},{"cell_type":"markdown","metadata":{"id":"dUiitxcfnGj8"},"source":["###Train"]},{"cell_type":"markdown","metadata":{"id":"Hp1kykx9kYBq"},"source":["####Task 1 and 2A\n","The possible labels are: 'humor', 'prejudice_woman', 'prejudice_lgbtiq', 'prejudice_inmigrant_race' and 'prejudice_gordofobia'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GWU0PtsnLJ9"},"outputs":[],"source":["EPOCHS = 5 #@param {type: \"integer\"}\n","model = BertClassifier()\n","LR = 1e-6 #@param {type: \"number\"}\n","label = 'humor' #@param {type: \"string\"}\n","train(model, train_set_humor, val_set_humor, label, LR, EPOCHS)"]},{"cell_type":"markdown","metadata":{"id":"ra2pYzhunU0y"},"source":["##Evaluation code"]},{"cell_type":"markdown","metadata":{"id":"98qN0hoVkptZ"},"source":["###Task 1 and 2A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFn119YoncRY"},"outputs":[],"source":["def evaluate(model, test_data, label):\n","\n","    test = Dataset(test_data, label)\n","\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","\n","        model = model.cuda()\n","\n","    total_acc_test = 0\n","    with torch.no_grad():\n","\n","        prediction = []\n","\n","        for test_input, test_label in test_dataloader:\n","\n","              test_label = test_label.to(device)\n","              mask = test_input['attention_mask'].to(device)\n","              input_id = test_input['input_ids'].squeeze(1).to(device)\n","\n","              output = model(input_id, mask)\n","\n","              acc = (output.argmax(dim=1) == test_label).sum().item()\n","              total_acc_test += acc\n","\n","              prediction.append(output.argmax(dim=1).cpu().numpy())\n","\n","    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n","    print(\"F1-Score Macro: \" + str(round(f1_score(dev_set[label], prediction, average='macro')*100, 2)))\n","    scores = f1_score(dev_set[label], prediction, average=None, labels=[0,1])\n","    for i in range(len(labels)):\n","        print(\"F-Score for class (\" + str(labels[i]) + \"): \" + str(round(scores[i]*100, 2)))\n","    print(\"F1-Score Positive: \" + str(round(scores[1]*100, 2)))\n","    print(\"F1-Score Negative: \" + str(round(scores[0]*100, 2)))"]},{"cell_type":"markdown","metadata":{"id":"YfcO9ShFywC4"},"source":["###Evaluate"]},{"cell_type":"markdown","metadata":{"id":"aSUTKrlDliLh"},"source":["####Task 1 and 2A\n","The possible labels are: 'humor', 'prejudice_woman', 'prejudice_lgbtiq', 'prejudice_inmigrant_race' and 'prejudice_gordofobia'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-7xfrhyyxyl"},"outputs":[],"source":["label = 'humor' #@param {type: \"string\"}\n","evaluate(model, dev_set, label)"]},{"cell_type":"markdown","metadata":{"id":"JQGNOyH0SIDU"},"source":["###Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQMWXM7xSL3D"},"outputs":[],"source":["PATH = '' #@param {type: \"string\"}\n","\n","model_name = '' #@param {type: \"string\"}\n","\n","torch.save(model.state_dict(), PATH + model_name)"]},{"cell_type":"markdown","metadata":{"id":"9iRXm3rP6c-y"},"source":["###Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2yUTUS76e3b"},"outputs":[],"source":["PATH = '' #@param {type: \"string\"}\n","model_name = '' #@param {type: \"string\"}\n","\n","model = BertClassifier()\n","model.load_state_dict(torch.load(PATH + model_name))\n","\n","evaluate_on_load = True #@param {type: \"boolean\"}\n","\n","if evaluate_on_load:\n","  label = 'humor' #@param {type: \"string\"}\n","  evaluate(model, dev_set, label)"]},{"cell_type":"markdown","metadata":{"id":"xqxIq0zfaO69"},"source":["##Inference"]},{"cell_type":"markdown","metadata":{"id":"6l8t5R-MERjn"},"source":["####Dataset Inference\n","This new dataset removes the label as it is not required."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uy_a2PL_EWeO"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","class DatasetInference(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_set):\n","        self.texts = [tokenizer(text,\n","                               padding='max_length', max_length = 512, truncation=True,\n","                                return_tensors=\"pt\") for text in data_set['tweet']]\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of inputs\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","        batch_texts = self.get_batch_texts(idx)\n","\n","        return batch_texts"]},{"cell_type":"markdown","metadata":{"id":"HbmeDLaZEeF-"},"source":["####Inference Code\n","The following code, given a dataframe with texts on the column 'tweet' and a model, will generate a list with the corresponding predictions for the texts presented."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyGi-B0tFZ4i"},"outputs":[],"source":["import numpy\n","\n","def inference(model, data):\n","  test = DatasetInference(data)\n","\n","  test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  if use_cuda:\n","\n","      model = model.cuda()\n","\n","  with torch.no_grad():\n","\n","      prediction = []\n","\n","      for test_input in test_dataloader:\n","\n","            mask = test_input['attention_mask'].to(device)\n","            input_id = test_input['input_ids'].squeeze(1).to(device)\n","\n","            output = model(input_id, mask)\n","\n","            prediction.append(output.argmax(dim=1).cpu().numpy())\n","\n","  return numpy.concatenate( prediction, axis=0 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3z-LkSSLIb5"},"outputs":[],"source":["prediction = inference(model,dev_set)"]},{"cell_type":"markdown","metadata":{"id":"QX4hm0PGLaSo"},"source":["Run the next cell with the the previous dataset on the second line to get the texts with their respective prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAyFDB53Ks7N"},"outputs":[],"source":["results = pd.DataFrame()\n","results['tweet'] = dev_set['tweet']\n","results['inference'] = prediction\n","\n","results"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO+dw9Nc6g94QdRrTZNNpJ6","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
